

<a name="ref1"></a>[1] Huang, S., et al. (2024). The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization. [arXiv:2403.17031](https://arxiv.org/abs/2403.17031).

<a name="ref2"></a>[2] Jones, A. (2020). Debugging RL, Without the Agonizing Pain. [https://andyljones.com/posts/rl-debugging.html](https://andyljones.com/posts/rl-debugging.html)

<a name="ref3"></a>[3] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe, R., Voss, C., Radford, A., Amodei, D., & Christiano, P. (2020). Learning to summarize from human feedback. *Advances in Neural Information Processing Systems*, 33, 3008-3021. [arXiv:2009.01325](https://arxiv.org/abs/2009.01325)

<a name="ref4"></a>[4] Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems*, 30. [arXiv:1706.03741](https://arxiv.org/abs/1706.03741)

<a name="ref5"></a>[5] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal Policy Optimization Algorithms. [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)

<a name="ref6"></a>[6] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, 35, 27730-27744. [arXiv:2203.02155](https://arxiv.org/abs/2203.02155)

<a name="ref7"></a>[7] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., & Kaplan, J. (2022). Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. [arXiv:2204.05862](https://arxiv.org/abs/2204.05862)

<a name="ref8"></a>[8] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *Advances in Neural Information Processing Systems*, 36. [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)

<a name="ref9"></a>[9] Ewalds, Z. Why is machine learning 'hard'? [https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html](https://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html)